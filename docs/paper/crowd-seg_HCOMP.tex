
\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai17}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}  
 \begin{document}
 
%Section Numbers
% Uncomment if you want to use section numbers
% and change the 0 to a 1 or 2
% \setcounter{secnumdepth}{0}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Tile-Based Quality Evaluation Methods for Crowdsourcing Image Segmentation}
\author{Authors removed for Anonymity}
\maketitle
\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}
\begin{itemize}
\item Most segmentation evaluation strategy people use are summarization/heuristic based, require ground truth, we come up with a more formal tile based model which does not require ground truth.
\item Summarization based method says this BB is good, that BB is bad, but doesn’t allow partial inclusion of a region
\item In the limit of small number of workers with noisy output, that means that if no BB is very good, we would only get an okay-ish BB. (c.f. dumbell example)
\item Tile based methods give us a way to allow partial inclusion of a region. 
\item This concept is similar to picking the majority voted region, but majority vote doesn’t account for worker qualities
\item With a worker error model + tile based scoring function, we can take make use of the best responses from many workers.
\item Discuss what are summarization based methods 
\item Explain its important to beat : 
\begin{itemize}
\item summarization-based methods
\item tile based majority vote 
\item Individual worker resposnes
\item Computer vision methods 
\end{itemize}
\end{itemize}

\section{Related Works}
\par Despite several large-scale efforts to collect image segmentation from crowds\cite{Lin2014,MartinFTM01,Torralba2010,pascal-voc-2012}, most have relied on simple area-based metrics to quantify their segmentation data quality. Research on quality evaluation of crowd segmentation often make use of heuristic approaches such as quantifying the user types and their clickstream behaviour to determine work quality\cite{Cabezas2015,Sameki2015}. Others have also made use of computer vision features to determine where the annotations should be located, in order to compare against the crowd annotations \cite{Vittayakorn2011,Russakovsky2015}.
\par \cite{Dawid1979} is the first work that used the EM algorithm to model an individual's biases and skills in the absence of ground truth data, by using a confusion matrix. \cite{OCWelinder2010} proposes a general model that separately models annotator quality and the biases applied to binary, multivalued and continuous annotations. \cite{MDWWelinder2010} develops a multidimensional concept of worker qualities and task difficulties by considering object-presence labelling as a noise generation process. The objective truth label is captured by a multidimensional quantity of task-specific measurements and deformed by worker and image related noise, the noisy vector obtained after this process is projected onto the vector of user expertise (which summarizes how well the user percieves each of these measurements), and finally the score is binarized into an inferred label. Many have extended this line of work beyond binary classifications by developing EM-like approaches that works on multiple-choice \cite{Karger2013} as well as free-form responses \cite{Lin2012}. 
\par However, even though EM algorithms assign probabilities regarding  \textit{how good a worker's bounding box is}, for the task of object segmentation, we are ultimately more interested the end goal of \textit{what is the best bounding box that we can get from these data}. Since these formal probabilistic models treats worker bounding box as the base quantity for modelling worker quality, the best bounding box that one could derive from such algorithm can only be as good as the best worker bounding box in the dataset.  Even though the annotation probabilities are sufficient for determining the best binary-labels, image information such as overlapping areas would be useful and not account for in these algorithms. We suspect that this is why many area-based metrics are still more commonly used in practice than EM approaches.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par $\Phi$ is a function that summarizes a given bounding box, where $\phi_i=\Phi(z_i)$ or $\phi_{ij}=\Phi(z_i,z_j)$.   The evaluation metrics can be grouped into three categories: 
\paragraph{Area-based: } These methods include precision, recall, area ratio or boundary complexity. These measures take into account the intersection, $\mathcal{I}=area(z_i\cup z_j)$, or union, $\mathcal{U}=area(z_i\cap z_j)$, between the user and the ground truth bounding boxes. Other heuristics include area ratio, ---- boundary proposed by ---- edge detectors, and number of control points in the user's bounding box \cite{Vittayakorn2011}.
 These methods examine how close is BB to regions of contrasts detected by CV algorithms (saliency maps, Bayesian Matting) or edge detectors. A major problem when implementing these methods is group and match CV regions to BB annotations, since CV methods often yield over-segmented regions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model}
\subsection{Tile Graph} Describe Tile based models. Briefly describe construction of tiles from BBs.
Clarify that our search space is tile combination formed by all the worker's tiles not the space of all possible coordinates. (i.e. we assume that a region can not be inside the BB if no workers bounded that region) 
\subsection{Definitions}
\subsection{Worker Error Model}
\subsection{Inference}
Describe assumptions on pdfs and inference process. (E and M steps). How are parameters in the models determined empirically.
\section{Experiments}
We collected data from Amazon Mechanical Turk where each HIT consisted of one annotation task for a specific pre-labelled object in the image. There is a total of 46 objects in 9 images from the MSCOCO dataset\cite{Lin2014}. These objects and images are chosen to represent a variety of image difficulty (based on object clutter-ness), potential logical error and level of ambiguity. The average number of objects annotations that each worker completed was 10.16. The average time to complete each HIT is 83.96 seconds and workers are compensated for 5 cents per HIT.  For each object, we collected annotations from a total of 40 independent workers.
\begin{itemize}
\item Data collection process
\item PR curves
\item show that our experiment works well on smaller, noisier datasets
\item Beat Baselines: 
\begin{itemize}
\item summarization-based methods
\item tile based majority vote 
\item Individual worker responses
\end{itemize}
\end{itemize}
\section{Conclusion}
\bibliographystyle{aaai}
\bibliography{reference}
\end{document}
