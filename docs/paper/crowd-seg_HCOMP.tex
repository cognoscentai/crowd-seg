
\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai17}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}  
 \begin{document}
 
%Section Numbers
% Uncomment if you want to use section numbers
% and change the 0 to a 1 or 2
% \setcounter{secnumdepth}{0}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Tile-Based Quality Evaluation Methods for Crowdsourcing Image Segmentation}
\author{Authors removed for Anonymity}
\maketitle
\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}
\begin{itemize}
\item Most segmentation evaluation strategy people use are summarization/heuristic based, require ground truth, we come up with a more formal tile based model which does not require ground truth.
\item Summarization based method says this BB is good, that BB is bad, but doesn’t allow partial inclusion of a region
\item In the limit of small number of workers with noisy output, that means that if no BB is very good, we would only get an okay-ish BB. (c.f. dumbell example)
\item Tile based methods give us a way to allow partial inclusion of a region. 
\item This concept is similar to picking the majority voted region, but majority vote doesn’t account for worker qualities
\item With a worker error model + tile based scoring function, we can take make use of the best responses from many workers.
\item Explain its important to beat : 
\begin{itemize}
\item summarization-based methods
\item tile based majority vote 
\item Individual worker resposnes
\item Computer vision methods 
\end{itemize}
\end{itemize}

\section{Related Works}
\par Despite several large-scale efforts to collect image segmentation from crowds\cite{Lin2014,MartinFTM01,Torralba2010,pascal-voc-2012}, most have relied on summarization-based metrics to quantify their segmentation data quality. Summarization-based scores are functions that measures the quality of a worker's bounding box given the ground truth. Common summarization methods include precision, recall, area ratio or number of control points in the bounding box~\cite{Vittayakorn2011}. While these are fairly discriminative measures of bounding box, the scoring calculation requires the use of ground truth to compare against. Other heuristic approaches that doesn't require ground truth segmentation include quantifying the user types and their click-stream behaviour to determine work quality\cite{Cabezas2015,Sameki2015} and comparing the worker responses to features extracted from computer vision algorithms\cite{Vittayakorn2011,Russakovsky2015}.
\par \cite{Dawid1979} is the first work that used the EM algorithm to model an individual's biases and skills in the absence of ground truth data, by using a confusion matrix. \cite{OCWelinder2010} proposes a general model that separately models annotator quality and the biases applied to binary, multivalued and continuous annotations. \cite{MDWWelinder2010} develops a multidimensional concept of worker qualities and task difficulties by considering object-presence labelling as a noise generation process. The objective truth label is captured by a multidimensional quantity of task-specific measurements and deformed by worker and image related noise, the noisy vector obtained after this process is projected onto the vector of user expertise (which summarizes how well the user percieves each of these measurements), and finally the score is binarized into an inferred label. Many have extended this line of work beyond binary classifications by developing EM-like approaches that works on multiple-choice \cite{Karger2013} as well as free-form responses \cite{Lin2012}, but these have not been directly applied to the task of object segmentation.
\par However, while EM algorithms assign probabilities regarding  \textit{how good a worker's bounding box is}, for the task of object segmentation, we are ultimately more interested the end goal of \textit{what is the best bounding box that we can get from these data}. Since these formal probabilistic models treats worker bounding box as the base quantity for modeling worker quality, the best bounding box that one could derive from such algorithm can only be as good as the best worker bounding box in the dataset.  Even though the annotation probabilities are sufficient for determining the best binary-labels, image information such as overlapping areas would be useful and not account for in these algorithms. We suspect that this is why many area-based metrics are still more commonly used in practice than EM approaches.
\section{Model}
\subsection{Tile Graph} Describe Tile based models. Briefly describe construction of tiles from BBs.
Clarify that our search space is tile combination formed by all the worker's tiles not the space of all possible coordinates. (i.e. we assume that a region can not be inside the BB if no workers bounded that region) 
\subsection{Definitions}
\subsection{Worker Error Model}
\subsection{Inference}
Describe assumptions on pdfs and inference process. (E and M steps). How are parameters in the models determined empirically.
\subsection{Optimization}
Explain heuristics used for avoiding to search through all T'. 
\section{Experiments}
We collected data from Amazon Mechanical Turk where each HIT consisted of one annotation task for a specific pre-labelled object in the image. There is a total of 46 objects in 9 images from the MSCOCO dataset~\cite{Lin2014}. These tasks represent a variety of image difficulty (based on object clutter-ness), potential logical error and level of ambiguity.   For each object, we collected annotations from a total of 40 independent workers.
\begin{itemize}
\item Data collection process
\item PR curves
\item show that our experiment works well on smaller, noisier datasets
\item Beat Baselines: 
\begin{itemize}
\item summarization-based methods
\item tile based majority vote 
\item Individual worker responses
\end{itemize}
\end{itemize}
\section{Conclusion}
\bibliographystyle{aaai}
\bibliography{reference}
\end{document}
